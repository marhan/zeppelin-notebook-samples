{
  "paragraphs": [
    {
      "text": "%md\n\n# S3 buckets in Spark / Zeppelin\n\nSpark is able to read from and save into S3 buckets. Hadoop is used to configure this functionality. The hyperlinks below will lead you to relevant backround information.\n\n- [Hadoop S3a Doc](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html#S3A)\n- [apache-spark-with-minio](https://github.com/minio/cookbook/blob/master/docs/apache-spark-with-minio.md)",
      "user": "anonymous",
      "dateUpdated": "2019-06-02 15:10:41.531",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch1\u003eS3 buckets in Spark / Zeppelin\u003c/h1\u003e\n\u003cp\u003eSpark is able to read from and save into S3 buckets. Hadoop is used to configure this functionality. The hyperlinks below will lead you to relevant backround information.\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\u003ca href\u003d\"https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html#S3A\"\u003eHadoop S3a Doc\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href\u003d\"https://github.com/minio/cookbook/blob/master/docs/apache-spark-with-minio.md\"\u003eapache-spark-with-minio\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559372312177_1934527590",
      "id": "20190601-065832_850901460",
      "dateCreated": "2019-06-01 06:58:32.177",
      "dateStarted": "2019-06-02 15:10:41.635",
      "dateFinished": "2019-06-02 15:10:41.706",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Read TEXT file",
      "text": "%spark\n\n\nval bankText \u003d sc.textFile(\"s3a://zeppelin/loremipsum.txt\")\n            \nbankText.collect().foreach(println)",
      "user": "anonymous",
      "dateUpdated": "2019-06-02 15:10:41.743",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "tableHide": false,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.\nbankText: org.apache.spark.rdd.RDD[String] \u003d s3a://zeppelin/loremipsum.txt MapPartitionsRDD[129] at textFile at \u003cconsole\u003e:54\n"
          }
        ]
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://3ffea045f327:4040/jobs/job?id\u003d29"
          ],
          "interpreterSettingId": "spark"
        }
      },
      "apps": [],
      "jobName": "paragraph_1559395845208_825612468",
      "id": "20190601-133045_372817550",
      "dateCreated": "2019-06-01 13:30:45.208",
      "dateStarted": "2019-06-02 15:10:41.794",
      "dateFinished": "2019-06-02 15:10:42.879",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Write TEXT file",
      "text": "%spark\n\n// create some data\nval stringRdd \u003d sc.parallelize(Seq(\"\u003d\u003d\u003d\u003e This is test content in a text file \u003c\u003d\u003d\u003d\"))\n\nstringRdd.saveAsTextFile(\"s3a://zeppelin/sample_output.txt\")",
      "user": "anonymous",
      "dateUpdated": "2019-06-02 15:10:42.900",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "title": true,
        "tableHide": false,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "stringRdd: org.apache.spark.rdd.RDD[String] \u003d ParallelCollectionRDD[130] at parallelize at \u003cconsole\u003e:55\n"
          }
        ]
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://3ffea045f327:4040/jobs/job?id\u003d30"
          ],
          "interpreterSettingId": "spark"
        }
      },
      "apps": [],
      "jobName": "paragraph_1559372512658_-1925184250",
      "id": "20190601-070152_1652146864",
      "dateCreated": "2019-06-01 07:01:52.659",
      "dateStarted": "2019-06-02 15:10:43.031",
      "dateFinished": "2019-06-02 15:11:04.445",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Delete TEXT file",
      "text": "%spark\nimport java.net.URI\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.fs.{Path, FileSystem}\n\nFileSystem.get(new URI(\"s3a://zeppelin\"), sc.hadoopConfiguration).delete(new Path(\"s3a://zeppelin/sample_output.txt\"), true)",
      "user": "anonymous",
      "dateUpdated": "2019-06-02 15:11:04.544",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "title": true,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import java.net.URI\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.fs.{Path, FileSystem}\nres39: Boolean \u003d true\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559484141805_-507453431",
      "id": "20190602-140221_552545451",
      "dateCreated": "2019-06-02 14:02:21.808",
      "dateStarted": "2019-06-02 15:11:04.705",
      "dateFinished": "2019-06-02 15:11:05.471",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Read CSV file",
      "text": "%spark\n\nval peopleCsvDf \u003d spark.read.format(\"com.databricks.spark.csv\")\n                .option(\"header\", \"true\")\n                .option(\"delimiter\",\";\")\n                .option(\"charset\", \"UTF-8\")\n                .option(\"treatEmptyValuesAsNulls\", \"True\")\n                .option(\"mode\", \"DROPMALFORMED\")\n                .load(\"s3a://zeppelin/people.csv\")\n                \n\npeopleCsvDf.printSchema\npeopleCsvDf.show",
      "user": "anonymous",
      "dateUpdated": "2019-06-02 15:11:05.507",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "title": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "root\n |-- name: string (nullable \u003d true)\n |-- age: string (nullable \u003d true)\n |-- job: string (nullable \u003d true)\n\n+-----+---+---------+\n| name|age|      job|\n+-----+---+---------+\n|Jorge| 30|Developer|\n|  Bob| 32|Developer|\n+-----+---+---------+\n\npeopleCsvDf: org.apache.spark.sql.DataFrame \u003d [name: string, age: string ... 1 more field]\n"
          }
        ]
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://3ffea045f327:4040/jobs/job?id\u003d31",
            "http://3ffea045f327:4040/jobs/job?id\u003d32"
          ],
          "interpreterSettingId": "spark"
        }
      },
      "apps": [],
      "jobName": "paragraph_1559372354554_-1810404279",
      "id": "20190601-065914_2031281241",
      "dateCreated": "2019-06-01 06:59:14.555",
      "dateStarted": "2019-06-02 15:11:05.674",
      "dateFinished": "2019-06-02 15:11:06.773",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Write CSV file",
      "text": "%spark\n\ncase class Record(key: Int, value: String)\nval recordDf \u003d spark.createDataFrame((1 to 100).map(i \u003d\u003e Record(i, s\"val_$i\")))\n                \nrecordDf.write\n    .format(\"com.databricks.spark.csv\")\n    .mode(\"overwrite\")\n    .save(\"s3a://zeppelin/sample_output.csv\")\n\n",
      "user": "anonymous",
      "dateUpdated": "2019-06-02 15:11:06.777",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "title": true,
        "tableHide": false,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "defined class Record\nrecordDf: org.apache.spark.sql.DataFrame \u003d [key: int, value: string]\n"
          }
        ]
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://3ffea045f327:4040/jobs/job?id\u003d33"
          ],
          "interpreterSettingId": "spark"
        }
      },
      "apps": [],
      "jobName": "paragraph_1559487068811_1863005427",
      "id": "20190602-145108_1864350521",
      "dateCreated": "2019-06-02 14:51:08.813",
      "dateStarted": "2019-06-02 15:11:06.941",
      "dateFinished": "2019-06-02 15:11:26.625",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Delete CSV file",
      "text": "%spark\nimport java.net.URI\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.fs.{Path, FileSystem}\n\nFileSystem.get(new URI(\"s3a://zeppelin\"), sc.hadoopConfiguration).delete(new Path(\"s3a://zeppelin/sample_output.csv\"), true)",
      "user": "anonymous",
      "dateUpdated": "2019-06-02 15:11:26.723",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import java.net.URI\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.fs.{Path, FileSystem}\nres42: Boolean \u003d true\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559487763612_-1554713414",
      "id": "20190602-150243_1419732682",
      "dateCreated": "2019-06-02 15:02:43.612",
      "dateStarted": "2019-06-02 15:11:26.816",
      "dateFinished": "2019-06-02 15:11:27.589",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Write Parquet file",
      "text": "%spark\n\ncase class Record(key: Int, value: String)\nval recordDf \u003d spark.createDataFrame((1 to 100).map(i \u003d\u003e Record(i, s\"val_$i\")))\n\nrecordDf.write.parquet(\"s3a://zeppelin/sample_output.parquet\")\n",
      "user": "anonymous",
      "dateUpdated": "2019-06-02 15:11:27.622",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "title": true,
        "tableHide": false,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "defined class Record\nrecordDf: org.apache.spark.sql.DataFrame \u003d [key: int, value: string]\n"
          }
        ]
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://3ffea045f327:4040/jobs/job?id\u003d34"
          ],
          "interpreterSettingId": "spark"
        }
      },
      "apps": [],
      "jobName": "paragraph_1559486111956_-762488362",
      "id": "20190602-143511_237558558",
      "dateCreated": "2019-06-02 14:35:11.956",
      "dateStarted": "2019-06-02 15:11:27.868",
      "dateFinished": "2019-06-02 15:11:46.537",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Delete parquet file from S3",
      "text": "%spark\nimport java.net.URI\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.fs.{Path, FileSystem}\n\nFileSystem.get(new URI(\"s3a://zeppelin\"), sc.hadoopConfiguration).delete(new Path(\"s3a://zeppelin/sample_output.parquet\"), true)",
      "user": "anonymous",
      "dateUpdated": "2019-06-02 15:11:46.603",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import java.net.URI\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.fs.{Path, FileSystem}\nres44: Boolean \u003d true\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559484365852_-191905094",
      "id": "20190602-140605_1097026455",
      "dateCreated": "2019-06-02 14:06:05.853",
      "dateStarted": "2019-06-02 15:11:46.867",
      "dateFinished": "2019-06-02 15:11:47.526",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\n",
      "user": "anonymous",
      "dateUpdated": "2019-06-02 15:11:47.572",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1559480793817_746979507",
      "id": "20190602-130633_755114115",
      "dateCreated": "2019-06-02 13:06:33.820",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "S3/Spark",
  "id": "2ECVDEN73",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "md:shared_process": [],
    "spark:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}